{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "from langfuse import Langfuse\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "langfuse = Langfuse(public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"), secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"), host=os.getenv(\"LANGFUSE_HOST\"))\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "data_path = cwd.parent / \"data\"\n",
    "documents_path = data_path / \"pliki_z_fabryki\"\n",
    "facts_path = documents_path / \"facts\"\n",
    "cache_path = documents_path / \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_files = list(documents_path.glob(\"*.txt\"))\n",
    "\n",
    "reports = [\n",
    "        {\"file_name\": report_file.name, \"content\": report_file.read_text()}\n",
    "        for report_file in report_files\n",
    "    ]\n",
    "\n",
    "fact_files = list(facts_path.glob(\"*.txt\"))\n",
    "\n",
    "facts = [\n",
    "    {\"file_name\": fact_file.name, \"content\": fact_file.read_text()}\n",
    "    for fact_file in fact_files\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract simple keywords\n",
    "\n",
    "No additional context knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_keywords_from_report(report_file_name: str, report_content: str) -> dict:\n",
    "    \"\"\"Generate keywords based only on report content without external knowledge.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    Twoją rolą jest analiza raportów bezpieczeństwa fabryki.\n",
    "    Zadanie polega na wybraniu słów kluczowych z raportu.\n",
    "    Słowa kluczowe muszą być zawsze w mianowniku.\n",
    "    W słowach kluczowych uwzględnij również numer sektora fabryki którego dotyczy raport. Informacja ta znajduje się w sekcji ze źródłem raportu.\n",
    "    Zamieniając sektor na słowo kluczowe zastosuj zawsze format: \"Sektor C1\", \"Sektor B2\" i podobne.\n",
    "    Każdy raport musi mieć informację o sektorze fabryki w słowach kluczowych.\n",
    "    Format wyjściowy powinien być w postaci listy słów kluczowych oddzielonych przecinkami. Nie znaków końca linii.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    <źródło raportu>{report_file_name}</źródło raportu>\n",
    "    <zawartość raportu>{report_content}</zawartość raportu>\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    keywords = result.choices[0].message.content\n",
    "    keywords = keywords.replace(\"\\n\", \"\")\n",
    "    keywords = keywords.split(\", \")\n",
    "    keywords = [keyword.strip() for keyword in keywords]\n",
    "\n",
    "    data = {\n",
    "        \"report_file_name\": report_file_name,\n",
    "        \"content\": report_content,\n",
    "        \"keywords\": keywords\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_with_keywords = [\n",
    "    generate_simple_keywords_from_report(report[\"file_name\"], report[\"content\"])\n",
    "    for report in reports\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_keywords_from_facts(facts_file_name: str, facts_content: str) -> dict:\n",
    "    \"\"\"Generate keywords based only on facts.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    Twoją rolą jest analiza faktów dostarczonych w formie notatki.\n",
    "    Zadanie polega na wybraniu słów kluczowych.\n",
    "    Słowa kluczowe muszą być zawsze w mianowniku.\n",
    "    Format wyjściowy powinien być w postaci listy słów kluczowych oddzielonych przecinkami. Nie znaków końca linii.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = facts_content\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    keywords = result.choices[0].message.content\n",
    "    keywords = keywords.replace(\"\\n\", \"\")\n",
    "    keywords = keywords.split(\", \")\n",
    "    keywords = [keyword.strip() for keyword in keywords]\n",
    "\n",
    "    data = {\n",
    "        \"facts_file_name\": facts_file_name,\n",
    "        \"facts_content\": facts_content,\n",
    "        \"keywords\": keywords\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_with_keywords = [\n",
    "    generate_simple_keywords_from_facts(fact[\"file_name\"], fact[\"content\"])\n",
    "    for fact in facts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in reports_with_keywords:\n",
    "    context_adjusted_keywords = []\n",
    "    for rpt_keyword in report[\"keywords\"]:\n",
    "        context_adjusted_keywords.append(rpt_keyword)\n",
    "        for fact in facts_with_keywords:\n",
    "            if rpt_keyword in fact[\"keywords\"]:\n",
    "                for fact_keyword in fact[\"keywords\"]:\n",
    "                    if fact_keyword not in context_adjusted_keywords:\n",
    "                        context_adjusted_keywords.append(fact_keyword)\n",
    "\n",
    "    report[\"context_adjusted_keywords\"] = context_adjusted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aidevs3.poligon import send\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.environ.get(\"AG3NTS_API_KEY\")\n",
    "url = f\"{os.environ.get(\"AG3NTS_CENTRALA_URL\")}/report\"\n",
    "\n",
    "result = {\n",
    "    report[\"report_file_name\"]: \", \".join(report[\"context_adjusted_keywords\"])\n",
    "    for report in reports_with_keywords\n",
    "}\n",
    "\n",
    "print(json.dumps(result))\n",
    "\n",
    "res = send(url, answer=result, apikey=key, task=\"dokumenty\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
