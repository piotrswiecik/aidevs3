{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions():\n",
    "    load_dotenv()\n",
    "    url = os.getenv(\"AG3NTS_CENTRALA_URL\") + \"/data/\" + os.getenv(\"AG3NTS_API_KEY\") + \"/arxiv.txt\"\n",
    "    r = httpx.get(url)\n",
    "    return r.text.split(\"\\n\")\n",
    "\n",
    "def trim_questions(questions) -> dict:\n",
    "    trimmed = dict()\n",
    "    for question in questions:\n",
    "        if len(question.split(\"=\")) == 2:\n",
    "            trimmed.update({question.split(\"=\")[0]: question.split(\"=\")[1]})\n",
    "    return trimmed\n",
    "\n",
    "def get_document():\n",
    "    load_dotenv()\n",
    "    url = os.getenv(\"AG3NTS_CENTRALA_URL\") + \"/dane/arxiv-draft.html\"\n",
    "    r = httpx.get(url)\n",
    "    return r.text\n",
    "\n",
    "html_document = get_document()\n",
    "questions = trim_questions(get_questions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_objects(document: str):\n",
    "    load_dotenv()\n",
    "    system_prompt = \"\"\"\n",
    "    <PURPOSE>\n",
    "    You are a HTML document parser.\n",
    "    Your task is to extract content objects from the document while taking into account the context of each object.\n",
    "    </PURPOSE>\n",
    "    <INSTRUCTIONS>\n",
    "    1. There are 3 types of content objects:\n",
    "    a. Text \n",
    "    - HTML block starting with a heading.\n",
    "    - The heading can be recognized on the basis of font size and formatting - it's typically visibly larger than the paragraph body.\n",
    "    b. Audio clip - a HTML link to an mp3 file.\n",
    "    c. Image\n",
    "    - a HTML element (often figure or img) with the src attribute pointing to an image url\n",
    "    - usually accompanied by a figcaption element with a caption\n",
    "    2. Find all content objects in the document and assign them numeric IDs and correct types.\n",
    "    3. Associate each content object with context. \n",
    "    - Context is a list of IDs of other content objects that are related to this object.\n",
    "    - Two content objects are related if they are in proximity to each other in the document - text next to image, text next to audio, text next to text and so on.\n",
    "    - Two objects are related if they are within 1-2 objects of each other in the document sequence.\n",
    "    4. Create content objects for all identified elements and return them in format described in the OUTPUT section.\n",
    "    \n",
    "    Important - the image might be place in the middle of text block. In such case, the text block shoud NOT be divided into two separate content objects.\n",
    "    Example of such case:\n",
    "    <h1>Heading</h1>\n",
    "    <p>Text before image</p>\n",
    "    <figure> # image block\n",
    "    <p>More text</p>\n",
    "    <h2>Subheading</h2>\n",
    "    In this case the text block spans from H1 to H2 and should be treated as one content object and the image should be treated as separate content object.\n",
    "\n",
    "    </INSTRUCTIONS>\n",
    "    <OUTPUT>\n",
    "    Output must be a valid JSON object with the following structure:\n",
    "    {\n",
    "    \"content_objects\": [\n",
    "        {\n",
    "        \"id\": <integer>,\n",
    "        \"type\": <\"text\" | \"audio\" | \"image\">,\n",
    "        \"context\": [<list of integers>],\n",
    "        \"content\": <string>,\n",
    "        \"caption\": <string> (only for image type) or null\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "\n",
    "    Each content object should represent one of the identified document elements.\n",
    "\n",
    "    Examples:\n",
    "    {\n",
    "    \"content_objects\": [\n",
    "        {\n",
    "        \"id\": 1,\n",
    "        \"type\": \"text\",\n",
    "        \"context\": [2, 3],\n",
    "        \"content\": \"This is a text content object.\",\n",
    "        \"caption\": null\n",
    "        },\n",
    "        {\n",
    "        \"id\": 2,\n",
    "        \"type\": \"audio\",\n",
    "        \"context\": [1],\n",
    "        \"content\": \"https://example.com/audio.mp3\",\n",
    "        \"caption\": null\n",
    "        },\n",
    "        {\n",
    "        \"id\": 3,\n",
    "        \"type\": \"image\",\n",
    "        \"context\": [1],\n",
    "        \"content\": \"https://example.com/image.jpg\",\n",
    "        \"caption\": \"This is a caption for the image taken from figcaption.\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    IMPORTANT RULES:\n",
    "    - Ensure the output is a valid JSON object.\n",
    "    - Use double quotes for all strings.\n",
    "    - Make sure the JSON is parsable without any syntax errors.\n",
    "    - Don't wrap the JSON into any other data structure or any markup. Yo must return only the JSON object.\n",
    "    </OUTPUT>\n",
    "    \"\"\"\n",
    "\n",
    "    ai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = ai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": document},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    content = content.lstrip(\"```json\\n\").rstrip(\"\\n```\")\n",
    "    return json.loads(content)[\"content_objects\"]\n",
    "\n",
    "data = extract_content_objects(html_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    for item in data:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            if not item[\"content\"].startswith(\"http\"):\n",
    "                item[\"content\"] = f\"{os.getenv('AG3NTS_CENTRALA_URL')}/dane/{item['content']}\"\n",
    "        if item[\"type\"] == \"audio\":\n",
    "            if not item[\"content\"].startswith(\"http\"):\n",
    "                item[\"content\"] = f\"{os.getenv('AG3NTS_CENTRALA_URL')}/dane/{item['content']}\"\n",
    "    return data\n",
    "\n",
    "processed_data = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in processed_data:\n",
    "    if d[\"type\"] == \"image\":\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in processed_data:\n",
    "    if d[\"type\"] == \"audio\":\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in processed_data:\n",
    "    if d[\"type\"] == \"text\":\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image(image_url, image_caption):\n",
    "    load_dotenv()\n",
    "    system_prompt = \"\"\"\n",
    "    <PURPOSE>\n",
    "    You are an image description generator. Your task is to generate a description for the image provided in the input taking also into account the provided caption.\n",
    "    </PURPOSE>\n",
    "    <INSTRUCTIONS>\n",
    "    - Focus on objects, landmarks, features.\n",
    "    - Don't describe atmosphere or mood.\n",
    "    - Use concise language.\n",
    "    - Analyze the caption and the image to generate a coherent description - the caption shouldn't contradict the image.\n",
    "    - The description should be 1-3 sentences long.\n",
    "    - The description should incorporate the information from caption.\n",
    "    </INSTRUCTIONS>\n",
    "    \"\"\"\n",
    "\n",
    "    ai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = ai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_url}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Describe the image. The caption is {image_caption}.\"\n",
    "                }\n",
    "            ]},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images = []\n",
    "\n",
    "for item in processed_data:\n",
    "    if item[\"type\"] == \"image\":\n",
    "        description = describe_image(item[\"content\"], image_caption=item[\"caption\"])\n",
    "        annotated_images.append({\"id\": item[\"id\"], \"description\": description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    load_dotenv()\n",
    "\n",
    "    ai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    audio = httpx.get(audio_url)\n",
    "\n",
    "    buf = io.BytesIO(audio.content)\n",
    "    buf.name = \"file.mp3\"\n",
    "\n",
    "    transcription = ai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=buf,\n",
    "    )\n",
    "    \n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_audio = []\n",
    "\n",
    "for item in processed_data:\n",
    "    if item[\"type\"] == \"audio\":\n",
    "        transcription = transcribe_audio(item[\"content\"])\n",
    "        transcribed_audio.append({\"id\": item[\"id\"], \"transcription\": transcription})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"annotated_images.pkl\", \"wb\") as f:\n",
    "    pickle.dump(annotated_images, f)\n",
    "\n",
    "with open(\"transcribed_audio.pkl\", \"wb\") as f:\n",
    "    pickle.dump(transcribed_audio, f)\n",
    "\n",
    "with open(\"processed_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(processed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = processed_data.copy()\n",
    "\n",
    "for item in _data:\n",
    "    if item[\"type\"] == \"text\": pass\n",
    "    if item[\"type\"] == \"image\":\n",
    "        for image in annotated_images:\n",
    "            if item[\"id\"] == image[\"id\"]:\n",
    "                item[\"content\"] = image[\"description\"]\n",
    "    if item[\"type\"] == \"audio\":\n",
    "        for audio in transcribed_audio:\n",
    "            if item[\"id\"] == audio[\"id\"]:\n",
    "                item[\"content\"] = audio[\"transcription\"]\n",
    "\n",
    "with open(\"final_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = f\"\"\"\n",
    "<PURPOSE>\n",
    "    You are a question answering model. Your task is to answer the question based on the provided information.\n",
    "    </PURPOSE>\n",
    "    <INSTRUCTIONS>\n",
    "    - The information is a collection of various objects. \n",
    "    - Each object has an ID, type, context pointers, and content.\n",
    "    - Each object can represent a text block, an image, or an audio clip. This is stated by the \"type\" field.\n",
    "    - The \"content\" field contains the actual content of the object. In case of images and audio clips, the content is a description or transcription.\n",
    "    - The \"context\" field is a list of IDs of other objects that are closely related to this object. \n",
    "    - Follow these relationships to analyze how data is interrelated but also take into account other objects in entire data.\n",
    "    - To answer the question, consider not only the content of the object but also its related objects as indicated by the \"context\" field.\n",
    "    - Explore the \"context\" field recursively to gather all relevant information, but prioritize direct relationships over distant ones.\n",
    "    - Focus on objects that directly contribute to answering the question, avoiding irrelevant details.\n",
    "    - For \"image\" objects, use the provided description or caption to infer information but also take into account the context pointers which may give important tips.\n",
    "    - For \"audio\" objects, use the transcription or description to extract relevant data.\n",
    "    - If conflicting information is found, prioritize the most directly related object in the context.\n",
    "    - If no relevant information exists, state this explicitly in the answer.\n",
    "    - Include names of objects, descriptions, and other relevant details in the answer.\n",
    "    - When searching for an acronym's explanation, consider not only the immediate context of objects but also recursively explore related objects.\n",
    "    - Use direct relationships first, and only if no explanation is found, explore more distant relationships.\n",
    "    - Always expand all acronyms and abbreviations using all informatation provided - make sure that you focus on finding the proper solution.\n",
    "\n",
    "    Answer the question based on entire provided information.\n",
    "    </INSTRUCTIONS>\n",
    "    <OUTPUT>\n",
    "    Your answer must be a single sentence.\n",
    "    Be accurate based on the provided context.\n",
    "    If no relevant information is found, respond with \"The information is not available in the provided context.\"\n",
    "    </OUTPUT>\n",
    "\n",
    "    IMPORTANT: all answers must be in Polish.\n",
    "    IMPORTANT: look carefully at all provided data objects before answering the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT_2 = f\"\"\"\n",
    "<PURPOSE>\n",
    "    You are a question answering model. Your task is to answer the question based on the provided information.\n",
    "    </PURPOSE>\n",
    "    <INSTRUCTIONS>\n",
    "    - The information is a collection of various objects. \n",
    "    - Each object has an ID, type, context pointers, and content.\n",
    "    - Each object can represent a text block, an image, or an audio clip. This is stated by the \"type\" field.\n",
    "    - The \"content\" field contains the actual content of the object. In case of images and audio clips, the content is a description or transcription.\n",
    "    - Focus on objects that directly contribute to answering the question, avoiding irrelevant details.\n",
    "    - For \"image\" objects, use the provided description or caption to infer information but also take into account the context pointers which may give important tips.\n",
    "    - For \"audio\" objects, use the transcription or description to extract relevant data.\n",
    "    - If conflicting information is found, prioritize the most directly related object in the context.\n",
    "    - If no relevant information exists, state this explicitly in the answer.\n",
    "    - Include names of objects, descriptions, and other relevant details in the answer.\n",
    "    - When searching for an acronym's explanation, consider not only the immediate context of objects but also recursively explore related objects.\n",
    "    - Use direct relationships first, and only if no explanation is found, explore more distant relationships.\n",
    "    - Always expand all acronyms and abbreviations using all informatation provided - make sure that you focus on finding the proper solution.\n",
    "\n",
    "    Answer the question based on entire provided information.\n",
    "    </INSTRUCTIONS>\n",
    "    <OUTPUT>\n",
    "    Your answer must be a single sentence.\n",
    "    Be accurate based on the provided context.\n",
    "    If no relevant information is found, respond with \"The information is not available in the provided context.\"\n",
    "    </OUTPUT>\n",
    "\n",
    "    IMPORTANT: all answers must be in Polish.\n",
    "    IMPORTANT: look carefully at all provided data objects before answering the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in _data:\n",
    "    if d[\"type\"] == \"text\":\n",
    "        print(d[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, data: list[dict]):\n",
    "    load_dotenv()\n",
    "    prompt = SYS_PROMPT\n",
    "\n",
    "    ai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = ai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"This is the question: {question}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"This is the data: {data}\"},\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Od czego pochodzą litery BNW w nazwie nowego modelu językowego?\"\n",
    "\n",
    "answer = answer_question(question, _data)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for i, q in questions.items():\n",
    "    answers.append((i, answer_question(q, _data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    question_id: answer for question_id, answer in answers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aidevs3.poligon import send\n",
    "\n",
    "url = os.environ.get(\"AG3NTS_CENTRALA_URL\") + \"/report\"\n",
    "task = \"arxiv\"\n",
    "api_key = os.environ.get(\"AG3NTS_API_KEY\")\n",
    "res = send(url, task, api_key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
