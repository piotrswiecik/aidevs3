import json
import logging
import os
from dataclasses import dataclass, field
from typing import Annotated, Generator, Optional, TypeVar
from uuid import uuid4
from abc import ABC, abstractmethod
from pydantic import BaseModel, Field
from typing import Optional

from openai import OpenAI
from openai.types.chat import ChatCompletion
from openai.resources.chat.completions import Stream, ChatCompletionChunk

from aidevs3.memory.prompts.knowledge import default_knowledge
from aidevs3.memory.prompts.structure import memory_structure
from aidevs3.memory.prompts.extractqueries import create_extract_queries_prompt
from dotenv import load_dotenv
from fastapi import Depends, FastAPI
from langfuse import Langfuse
from langfuse.client import StatefulTraceClient
from openai.types.chat import ChatCompletionMessageParam

load_dotenv()


app = FastAPI()

langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_API_KEY"),
    secret_key=os.getenv("LANGFUSE_PRIVATE_API_KEY"),
    host=os.getenv("LANGFUSE_HOST"),
)


class AIService(ABC):
    """Represents AI completion service interface. Can be implemented for different LLM providers."""

    @abstractmethod
    def completion(
        self,
        messages: list[ChatCompletionMessageParam],
        model: Optional[str] = None,
        stream: bool = False,
        json_mode: bool = False,
        max_tokens: Optional[int] = None,
    ) -> ChatCompletion | Stream[ChatCompletionChunk]:
        """
        Returns a string or a generator of strings based on the stream flag.
        If json_mode is true, the response is parsed as a JSON object.

        Args:
            messages: List of messages to be sent to the LLM.
            model: Optional model name e.g. "gpt-4o-mini".
            stream: If true, the response is streamed.
            json_mode: If true, the response is parsed as a JSON object.
            max_tokens: Optional maximum number of tokens in the response.
        Returns:
            String or a generator of strings.
        """
        pass

    @abstractmethod
    def parse_json_response(self, response: ChatCompletion):
        """
        Parses the model stringified response into a JSON object.
        """
        pass


class OpenAIServiceImpl(AIService):
    """Implementation of AI service for OpenAI."""

    def __init__(self):
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        self.logger = logging.getLogger("openai_service")

    def completion(
        self,
        messages: list[ChatCompletionMessageParam],
        model: Optional[str] = None,
        stream: bool = False,
        json_mode: bool = False,
        max_tokens: Optional[int] = None,
    ) -> ChatCompletion | Stream[ChatCompletionChunk]:
        response = self.client.chat.completions.create(
            model=model or "gpt-4o-mini",
            messages=messages,
            stream=stream,
            response_format={"type": "json_object"} if json_mode else None,
            max_tokens=max_tokens,
        )

        return response
    
    def parse_json_response(self, response: ChatCompletion):
        try:
            content = response.choices[0].message.content # this comes as string from openai json mode format
            return json.loads(content)
        except (AttributeError, KeyError, IndexError) as e:
            self.logger.error(e)
            raise Exception(f"Invalid JSON response structure")


class AssistantService:
    def __init__(self, ai_service: AIService):
        self.ai_service = ai_service

    async def extract_queries(
        self, messages: list[ChatCompletionMessageParam], trace: StatefulTraceClient
    ) -> list[str]:
        # create generation step in langfuse logging
        generation = trace.generation(name="Extract Queries", input=messages)

        # memory_structure -> describes how agent memory is structured (categories, subcategories, etc.)
        # default_knowledge -> describes what the agent knows about the world (name, personality, etc.)

        # thread contains all user messages + system message generated by agent based on memory_structure and default_knowledge

        thread = [
            {
                "role": "system",
                "content": create_extract_queries_prompt(
                    memory_structure, default_knowledge
                ),
            },
            *messages,
        ]

        thinking = self.ai_service.completion(thread, json_mode=True)
        result = self.ai_service.parse_json_response(thinking)

        # store result in langfuse
        try: 
            usage = {
                "prompt_tokens": thinking.usage.prompt_tokens,
                "completion_tokens": thinking.usage.completion_tokens,
                "total_tokens": thinking.usage.total_tokens,
            }
        except AttributeError:
            usage = None
        finally:
            generation.update(output=json.dumps(result), model=thinking.model, usage=usage)

        return result


class ChatMessage(BaseModel):
    role: str = "user"
    content: str


class ChatRequest(BaseModel):
    messages: list[ChatMessage]
    conversation_id: Optional[str] = Field(default_factory=lambda: str(uuid4()))


@app.post("/chat")
async def chat(
    request: ChatRequest,
):
    assistant_service = AssistantService(OpenAIServiceImpl())

    # get only user messages
    messages = list(filter(lambda i: i.role == "user", request.messages))

    # start logging
    trace_name = str(messages[0].content) if len(messages) > 0 else ""
    trace = langfuse.trace(
        session_id=request.conversation_id,
        name=trace_name[:45],
    )

    # use LLM agent to prepare queries that can be useful to recall information from memory
    # example of such queries: "profiles:basic Who is Alice?", "profiles:basic Who is Adam?"
    # generally they contain a category:subcategory and a query string
    queries = await assistant_service.extract_queries(request.messages, trace)

    return queries
